replace 185
185 -- 610 - 761,
196 -- 196 - >720
209 -- 429 - >762
211 --  452---839
212 -- 831 - 833
213 -- 536 - 778
214 -- 195  - 823
215 --655  - 653


230   346 -- 544



num_pages= [838,680,935,720,639,787,765,776,730,681,707,777,798,645,790,741,654,760,581,658,772,766,757,782,660,704,
610,780,736,737,750,923,730,706,668,692,196,753,845,686,
713,339,604,787,751,765,796,858,429,877,452,831,536,195,655,662,692,702,873,733,713,689,659,774,815,734,727,346,284]

188  737 items but 736


curr = -100
collection_header = []
for index,row in df.iterrows():
    if(row["Collection"]!= curr):
        collection_header.append(row["Name"])
    else:
        collection_header.append("")
    curr = row["Collection"]
df["Collection_Head"] = collection_header
df.set_index([df.index,"Collection_Head"],inplace=True)






1853, Septe-1857
1862-1864
1919
1862-65 
not d 1891-1895
not MAR 17 1941
not out of range
73 - -    Name.lower(){.}{0,3} the digit follows it no,
19th - 20th Centuries



papers, screen plays, daybooks, scrapbooks



# import nltk
# bigrams = nltk.collocations.BigramAssocMeasures()
# # bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(text_tokens)
# function_words_single = ["the", "of", "and", "to", "a", "in", "i", "he", "that", "was", "it", "his", "you", "with", "as", "for", "had", "is", "her", "not", "but", "at", "on", "she", "be", "have", "by", "which", "him", "they", "this", "from", "all", "were", "my", "we", "one", "so", "said", "me", "there", "or", "an", "are", "no", "would", "their", "if", "been", "when", "do", "who", "what", "them", "will", "out", "up", "then", "more", "could", "into", "man", "now", "some", "your", "very", "did", "has", "about", "time", "can", "little", "than", "only", "upon", "its", "any", "other", "see", "our", "before", "two", "know", "over", "after", "down", "made", "should", "these", "must", "such", "much", "us", "old", "how", "come", "here", "never", "may", "first", "where", "go", "s", "came", "men", "way", "back", "himself", "own", "again", "say", "day", "long", "even", "too", "think", "might", "most", "through", "those", "am", "just", "make", "while", "went", "away", "still", "every", "without", "many", "being", "take", "last", "shall", "yet", "though", "nothing", "get", "once", "under", "same", "off", "another", "let", "tell", "why", "left", "ever", "saw", "look", "seemed", "against", "always", "going", "few", "got", "something", "between", "sir", "thing", "also", "because", "yes", "each", "oh", "quite", "both", "almost", "soon", "however", "having", "t", "whom", "does", "among", "perhaps", "until", "began", "rather", "herself", "next", "since", "anything", "myself", "nor", "indeed", "whose", "thus", "along", "others", "till", "near", "certain", "behind", "during", "alone", "already", "above", "often", "really", "within", "used", "use", "itself", "whether", "around", "second", "across", "either", "towards", "became", "therefore", "able", "sometimes", "later", "else", "seems", "ten", "thousand", "don", "certainly", "ought", "beyond", "toward", "nearly", "although", "past", "seem", "mr", "mrs", "dr", "thou", "except", "none", "probably", "neither", "saying", "ago", "ye", "yourself", "getting", "below", "quickly", "beside", "besides", "especially", "thy", "thee", "d", "unless", "three", "four", "five", "six", "seven", "eight", "nine", "hundred", "million", "billion", "third", "fourth", "fifth", "sixth", "seventh", "eighth", "ninth", "tenth", "amp", "m", "re", "u", "via", "ve", "ll", "th", "lol", "pm", "things", "w", "didn", "doing", "doesn", "r", "gt", "n", "st", "lot", "y", "im", "k", "isn", "ur", "hey", "yeah", "using", "vs", "dont", "ok", "v", "goes", "gone", "lmao", "happen", "wasn", "gotta", "nd", "okay", "aren", "wouldn", "couldn", "cannot", "omg", "non", "inside", "iv", "de", "anymore", "happening", "including", "shouldn", "yours",]
# len(function_words_single)

#bigrams
bigram_freq = bigramFinder.ngram_fd.items()
bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)
bigramFreqTable


# from gensim.models.phrases import Phrases, Phraser
# def fit_phrases():

#     # if non_eng == False:
#     common_terms = function_words_single
#     # else:
#     #     common_terms = []
#     phrases = Phrases(
#                     sentences = ["he","she"],   # clean text
#                     min_count = 25, 
#                     threshold = 0.70,
#                     scoring = "npmi",
#                     max_vocab_size = 100000000,
#                     delimiter = b"_",
#                     connector_words= common_terms
#                     )
#     print(phrases)
#     phrases = Phraser(str(phrases))
#     print(phrases.phrasegrams)
# fit_phrases()



charleston earthquake 
[ 1295,  3077, 11346, 12237, 12465, 12498, 12499, 12820,
            16096, 21313, 21609, 23873, 24542,
            29217, 30480, 32219, 35513, 36175, 37469, 37798, 40147, 41434,
            43150, 44239, 44405, 47110]


[1296,]